<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Transformer相关学习 | lhldudu's blog</title><meta name="author" content="Harrisonls2004"><meta name="copyright" content="Harrisonls2004"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Transformer简介为什么需要Transformer模型背景与动机 随着自然语言处理（NLP）技术的快速发展，传统的序列模型逐渐暴露出明显的局限性： 传统RNN&#x2F;LSTM的瓶颈  计算效率低：必须按时间步顺序处理，无法并行计算，训练速度慢 长期依赖问题：虽然LSTM通过门控机制缓解了梯度消失，但对于超长序列仍然难以捕捉远距离依赖关系 信息传递路径长：信息需要逐步传递，容易造成信息损">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer相关学习">
<meta property="og:url" content="https://harrisonls2004.github.io/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="lhldudu&#39;s blog">
<meta property="og:description" content="Transformer简介为什么需要Transformer模型背景与动机 随着自然语言处理（NLP）技术的快速发展，传统的序列模型逐渐暴露出明显的局限性： 传统RNN&#x2F;LSTM的瓶颈  计算效率低：必须按时间步顺序处理，无法并行计算，训练速度慢 长期依赖问题：虽然LSTM通过门控机制缓解了梯度消失，但对于超长序列仍然难以捕捉远距离依赖关系 信息传递路径长：信息需要逐步传递，容易造成信息损">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://harrisonls2004.github.io/img/Transformer.jpg">
<meta property="article:published_time" content="2025-03-28T05:00:00.000Z">
<meta property="article:modified_time" content="2026-01-28T10:33:59.135Z">
<meta property="article:author" content="Harrisonls2004">
<meta property="article:tag" content="Trasformer">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="扩展学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://harrisonls2004.github.io/img/Transformer.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Transformer相关学习",
  "url": "https://harrisonls2004.github.io/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/",
  "image": "https://harrisonls2004.github.io/img/Transformer.jpg",
  "datePublished": "2025-03-28T05:00:00.000Z",
  "dateModified": "2026-01-28T10:33:59.135Z",
  "author": [
    {
      "@type": "Person",
      "name": "Harrisonls2004",
      "url": "https://Harrisonls2004.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/logo.png"><link rel="canonical" href="https://harrisonls2004.github.io/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css?v=5.5.4-b1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.7/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":true,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":100,"languages":{"author":"作者: Harrisonls2004","link":"链接: ","source":"来源: lhldudu's blog","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"已切换为繁体中文","cht_to_chs":"已切换为简体中文","day_to_night":"已切换为深色模式","night_to_day":"已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Transformer相关学习',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="/css/universe.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div class="bg-animation" id="web_bg" style="background-image: url(/img/bg.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">30</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 演唱会</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/Transformer.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/logo.png" alt="Logo"><span class="site-name">lhldudu's blog</span></a><a class="nav-page-title" href="/"><span class="site-name">Transformer相关学习</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 演唱会</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Transformer相关学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-03-28T05:00:00.000Z" title="发表于 2025-03-28 13:00:00">2025-03-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-01-28T10:33:59.135Z" title="更新于 2026-01-28 18:33:59">2026-01-28</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/">学习记录</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">9.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>39分钟</span></span><span class="post-meta-separator">|</span><span id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span class="waline-pageview-count" data-path="/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/#post-comment"><span class="waline-comment-count" data-path="/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h2 id="Transformer简介"><a href="#Transformer简介" class="headerlink" title="Transformer简介"></a>Transformer简介</h2><h3 id="为什么需要Transformer模型"><a href="#为什么需要Transformer模型" class="headerlink" title="为什么需要Transformer模型"></a>为什么需要Transformer模型</h3><p><strong>背景与动机</strong></p>
<p>随着自然语言处理（NLP）技术的快速发展，传统的序列模型逐渐暴露出明显的局限性：</p>
<p><strong>传统RNN&#x2F;LSTM的瓶颈</strong></p>
<ul>
<li><strong>计算效率低</strong>：必须按时间步顺序处理，无法并行计算，训练速度慢</li>
<li><strong>长期依赖问题</strong>：虽然LSTM通过门控机制缓解了梯度消失，但对于超长序列仍然难以捕捉远距离依赖关系</li>
<li><strong>信息传递路径长</strong>：信息需要逐步传递，容易造成信息损失</li>
</ul>
<p><strong>Transformer的创新突破</strong></p>
<p>2017年，Google团队在论文《Attention is All You Need》中提出了Transformer架构，完全抛弃了循环结构，转而采用：</p>
<ul>
<li><strong>自注意力机制（Self-Attention）</strong>：允许模型直接建模序列中任意两个位置之间的关系</li>
<li><strong>多头注意力（Multi-Head Attention）</strong>：从多个角度并行捕捉不同的语义信息</li>
<li><strong>并行计算</strong>：所有位置可以同时处理，大幅提升训练效率</li>
</ul>
<h3 id="Transformer模型的核心特点"><a href="#Transformer模型的核心特点" class="headerlink" title="Transformer模型的核心特点"></a>Transformer模型的核心特点</h3><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/0d386847-d62e-4aaa-bcef-06ec8499ab15.png" alt="89a67ca3e34e8114dcf669f7c33fb3dd_720.png"></p>
<h3 id="Transformer模型的核心特点-1"><a href="#Transformer模型的核心特点-1" class="headerlink" title="Transformer模型的核心特点"></a>Transformer模型的核心特点</h3><p><strong>整体架构</strong></p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/0d386847-d62e-4aaa-bcef-06ec8499ab15.png" alt="Transformer架构图"></p>
<p>Transformer采用经典的<strong>编码器-解码器（Encoder-Decoder）</strong>架构：</p>
<ul>
<li><strong>编码器（Encoder）</strong>：负责理解输入序列，提取语义特征</li>
<li><strong>解码器（Decoder）</strong>：基于编码器的输出，生成目标序列</li>
<li><strong>层数设计</strong>：原始论文中编码器和解码器各包含6层相同结构的子层</li>
</ul>
<p><strong>三大核心特性</strong></p>
<p><strong>1. 自注意力机制（Self-Attention）</strong></p>
<p>这是Transformer最核心的创新。传统模型只能关注局部上下文或固定窗口内的信息，而自注意力机制允许：</p>
<ul>
<li>每个位置可以<strong>直接</strong>关注到序列中的所有其他位置</li>
<li>动态计算不同位置之间的相关性权重</li>
<li>无视距离远近，直接建模长距离依赖关系</li>
</ul>
<p><strong>2. 并行计算能力</strong></p>
<p>与RNN&#x2F;LSTM的顺序处理不同，Transformer具有天然的并行性：</p>
<ul>
<li>所有位置的表示可以<strong>同时计算</strong>，无需等待前一时刻的结果</li>
<li>充分利用现代GPU的并行计算能力</li>
<li>训练速度相比RNN提升数倍甚至数十倍</li>
</ul>
<p><strong>3. 灵活的任务适应性</strong></p>
<p>Transformer的架构设计极具通用性：</p>
<ul>
<li><strong>可变长度输入</strong>：通过位置编码支持任意长度的序列</li>
<li><strong>可扩展性强</strong>：通过增加层数和注意力头数提升模型容量</li>
<li><strong>多任务适用</strong>：不仅限于机器翻译，还广泛应用于文本生成、分类、问答、摘要等任务</li>
<li><strong>迁移学习基础</strong>：成为BERT、GPT等预训练模型的基础架构</li>
</ul>
<h3 id="内部机制概述"><a href="#内部机制概述" class="headerlink" title="内部机制概述"></a>内部机制概述</h3><p><strong>Transformer的整体工作流程</strong></p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/image.png" alt="Transformer工作流程图1"></p>
<p>上图展示了Transformer的基本输入输出流程。输入序列经过嵌入层和位置编码后，进入编码器进行特征提取。</p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/image%201.png" alt="编码器内部结构"></p>
<p>编码器通过多头注意力机制捕捉序列内部的依赖关系，然后通过前馈网络进行特征变换。</p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/image%202.png" alt="解码器工作机制"></p>
<p>解码器在生成目标序列时，不仅关注自身已生成的内容（自注意力），还需要关注编码器的输出（交叉注意力）。</p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/image%203.png" alt="注意力机制可视化"></p>
<p>注意力权重的可视化展示了模型如何在不同位置之间建立关联。颜色越深表示注意力权重越大。</p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/image%204.png" alt="多层堆叠效果"></p>
<p>通过堆叠多层编码器和解码器，模型能够学习到从低级到高级的特征表示。</p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/image%205.png" alt="完整架构总览"></p>
<p>这是Transformer的完整架构图，清晰展示了数据流动的全过程：从输入嵌入、位置编码，经过多层编码器和解码器，最终输出预测结果。</p>
<h3 id="Encoder（编码器）"><a href="#Encoder（编码器）" class="headerlink" title="Encoder（编码器）"></a>Encoder（编码器）</h3><p><strong>编码器的组成与作用</strong></p>
<p>编码器由<strong>多头注意力机制（Multi-Head Attention）</strong>和<strong>前馈神经网络（Feed Forward Network）</strong>两个核心模块构成。原始Transformer使用6层编码器堆叠，每层都包含相同的结构但参数独立。</p>
<p><strong>分层特征提取</strong></p>
<ul>
<li><strong>底层</strong>：关注词汇的基本关系和短期依赖，识别基本的语法模式</li>
<li><strong>中间层</strong>：识别更长范围的依赖关系，捕捉词语之间的语义关联</li>
<li><strong>高层</strong>：关注整个句子的结构和深层语义，如句子级别的语法关系或情感倾向</li>
</ul>
<p>通过多层结构，Transformer在每一层中对信息进行渐进的抽象和加工，最终获得高层次的、能够适应各种任务的表示。</p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/image%206.png" alt="编码器结构示意图1"></p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/image%207.png" alt="编码器数据流动"></p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/image%208.png" alt="编码器层级结构"></p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/image%209.png" alt="编码器输出特征"></p>
<h3 id="Decoder（解码器）"><a href="#Decoder（解码器）" class="headerlink" title="Decoder（解码器）"></a>Decoder（解码器）</h3><p><strong>解码器的结构特点</strong></p>
<p>解码器同样由6层堆叠而成，但每层包含<strong>三个</strong>子模块（比编码器多一个）：</p>
<ol>
<li><strong>带掩码的多头注意力层</strong>：采用Masked操作，确保生成过程的自回归特性</li>
<li><strong>编码器-解码器注意力层</strong>：K和V矩阵来自编码器输出，Q矩阵来自解码器自身</li>
<li><strong>前馈神经网络层</strong>：与编码器中的结构相同</li>
</ol>
<p><strong>掩码机制（Mask）</strong></p>
<p>Transformer中使用两种掩码：</p>
<ul>
<li><strong>Padding Mask</strong>：在较短序列后填充0，避免模型关注填充位置</li>
<li><strong>Sequence Mask</strong>：确保解码时只能依赖当前时刻之前的输出，不能”看到未来”</li>
</ul>
<p><strong>编码器输出矩阵C</strong></p>
<p>编码器的输出是一个 n×d 的矩阵，其中：</p>
<ul>
<li>n：输入序列长度</li>
<li>d：特征维度（由隐藏层维度决定）</li>
</ul>
<p>该矩阵包含了输入序列每个元素的上下文嵌入表示，作为解码器交叉注意力的键值对。</p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/image%2010.png" alt="解码器结构图"></p>
<h2 id="Transformer模型实现"><a href="#Transformer模型实现" class="headerlink" title="Transformer模型实现"></a>Transformer模型实现</h2><p>本节将详细介绍Transformer各个组件的PyTorch实现，包括输入嵌入、位置编码、注意力机制等核心模块。</p>
<h3 id="输入嵌入（Input-Embeddings）"><a href="#输入嵌入（Input-Embeddings）" class="headerlink" title="输入嵌入（Input Embeddings）"></a>输入嵌入（Input Embeddings）</h3><p><strong>功能说明</strong></p>
<p>将离散的token ID转换为连续的向量表示。例如，将句子”Your cat is a lovely cat”转换为512维向量序列。</p>
<p><strong>实现细节</strong></p>
<ul>
<li>使用PyTorch的<code>nn.Embedding</code>层实现token到向量的映射</li>
<li>每个token ID对应一个固定的512维向量（可学习参数）</li>
<li>输出向量乘以√d_model进行缩放（论文3.4节要求）</li>
</ul>
<p><strong>缩放的数学原理</strong></p>
<p>缩放操作使嵌入向量的L2范数与维度无关，避免在后续与位置编码相加时某个分量过大。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InputEmbeddings</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;词嵌入层：将token ID转换为连续向量&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, vocab_size: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            d_model: 嵌入向量维度（如512）</span></span><br><span class="line"><span class="string">            vocab_size: 词汇表大小（如10000）</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model</span><br><span class="line">        <span class="variable language_">self</span>.vocab_size = vocab_size</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(vocab_size, d_model)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播</span></span><br><span class="line"><span class="string">        输入: (batch, seq_len) - token IDs</span></span><br><span class="line"><span class="string">        输出: (batch, seq_len, d_model) - 嵌入向量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.embedding(x) * math.sqrt(<span class="variable language_">self</span>.d_model)</span><br></pre></td></tr></table></figure>

<h3 id="位置编码（Positional-Encoding）"><a href="#位置编码（Positional-Encoding）" class="headerlink" title="位置编码（Positional Encoding）"></a>位置编码（Positional Encoding）</h3><p><strong>为什么需要位置编码</strong></p>
<p>Transformer的注意力机制本身是位置无关的（permutation invariant），无法区分”I ate an apple”和”An apple ate me”这类语序不同的句子。因此需要显式地为模型注入位置信息。</p>
<p><strong>实现方式</strong></p>
<p>使用预定义的数学公式生成位置编码，而非可学习参数：</p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/aa74f29784f5c9263d0d71c0bca2975.png" alt="位置编码公式"></p>
<p>其中：</p>
<ul>
<li>pos：单词在序列中的位置（0-based索引）</li>
<li>i：维度索引（0 ≤ i &lt; d_model&#x2F;2）</li>
<li>偶数维度使用正弦函数，奇数维度使用余弦函数</li>
</ul>
<p><strong>融合方式</strong></p>
<p>位置编码与词嵌入<strong>逐元素相加</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最终表示 = 词嵌入 + 位置编码</span><br></pre></td></tr></table></figure>

<p><strong>关键特性</strong></p>
<ul>
<li><strong>相对位置感知</strong>：正弦&#x2F;余弦函数的周期性使模型能捕捉相对位置关系</li>
<li><strong>可扩展性</strong>：允许处理比训练时更长的序列</li>
<li><strong>数值平衡</strong>：确保位置编码值域与词嵌入值域相匹配</li>
</ul>
<p><strong>可视化示例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设 d_model=4, seq_len=3</span></span><br><span class="line">词嵌入 = [[<span class="number">0.2</span>, <span class="number">1.1</span>, -<span class="number">0.5</span>, <span class="number">0.8</span>],   <span class="comment"># &quot;Hello&quot;</span></span><br><span class="line">         [<span class="number">0.7</span>, -<span class="number">0.3</span>, <span class="number">1.2</span>, <span class="number">0.4</span>],   <span class="comment"># &quot;World&quot;</span></span><br><span class="line">         [<span class="number">0.9</span>, <span class="number">0.5</span>, -<span class="number">0.1</span>, <span class="number">1.0</span>]]   <span class="comment"># &quot;!&quot;</span></span><br><span class="line"></span><br><span class="line">位置编码 = [[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>],     <span class="comment"># 位置0</span></span><br><span class="line">          [<span class="number">0.84</span>, <span class="number">0.54</span>, <span class="number">0.002</span>, <span class="number">1.0</span>],  <span class="comment"># 位置1</span></span><br><span class="line">          [<span class="number">0.91</span>, -<span class="number">0.42</span>, <span class="number">0.003</span>, <span class="number">0.99</span>]] <span class="comment"># 位置2</span></span><br><span class="line"></span><br><span class="line">最终表示 = 词嵌入 + 位置编码</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;位置编码层（基于Attention is All You Need论文3.5节）&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, seq_len: <span class="built_in">int</span>, dropout: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            d_model: 模型维度（必须与词嵌入维度相同）</span></span><br><span class="line"><span class="string">            seq_len: 预设的最大序列长度</span></span><br><span class="line"><span class="string">            dropout: dropout概率</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model</span><br><span class="line">        <span class="variable language_">self</span>.seq_len = seq_len</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 预计算位置编码矩阵 (seq_len, d_model)</span></span><br><span class="line">        pe = torch.zeros(seq_len, d_model)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 生成位置索引 [0, 1, 2, ..., seq_len-1]</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, seq_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)  <span class="comment"># (seq_len, 1)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算频率项分母（对数空间计算，数值更稳定）</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">       </span><br><span class="line">        <span class="comment"># 偶数维度使用正弦，奇数维度使用余弦</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 添加batch维度 (1, seq_len, d_model)</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 注册为buffer（随模型保存但不参与梯度计算）</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播</span></span><br><span class="line"><span class="string">        输入/输出: (batch_size, seq_len, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 截取与输入长度匹配的位置编码并相加</span></span><br><span class="line">        x = x + (<span class="variable language_">self</span>.pe[:, :x.shape[<span class="number">1</span>], :]).requires_grad_(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.dropout(x)</span><br></pre></td></tr></table></figure>

<h3 id="层归一化（Layer-Normalization）"><a href="#层归一化（Layer-Normalization）" class="headerlink" title="层归一化（Layer Normalization）"></a>层归一化（Layer Normalization）</h3><p><strong>核心原理</strong></p>
<p>层归一化对每个样本独立进行标准化处理，分为三步：</p>
<ol>
<li><strong>计算统计量</strong>：为每个样本单独计算所有特征的均值和方差</li>
<li><strong>标准化</strong>：将特征值转换为均值为0、方差为1的标准分布</li>
<li><strong>可学习变换</strong>：通过gamma（缩放）和beta（偏移）参数调整输出</li>
</ol>
<p><strong>关键特性</strong></p>
<ul>
<li>处理不同长度文本时更稳定</li>
<li>与Transformer的残差连接配合良好</li>
<li>训练和推理时行为一致（不依赖batch统计量）</li>
</ul>
<p><strong>数值稳定性</strong></p>
<p>使用epsilon（ε &#x3D; 10⁻⁶）避免除零错误，确保计算稳定性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNormalization</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;层归一化（参考论文《Layer Normalization》）&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features: <span class="built_in">int</span>, eps: <span class="built_in">float</span>=<span class="number">10</span>**-<span class="number">6</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            features: 输入特征维度（对应d_model）</span></span><br><span class="line"><span class="string">            eps: 防止除零的小常数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.eps = eps</span><br><span class="line">        <span class="variable language_">self</span>.alpha = nn.Parameter(torch.ones(features))   <span class="comment"># 缩放参数γ</span></span><br><span class="line">        <span class="variable language_">self</span>.bias = nn.Parameter(torch.zeros(features))   <span class="comment"># 偏移参数β</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播: (x-μ)/σ * α + β</span></span><br><span class="line"><span class="string">        输入/输出: (batch_size, seq_len, features)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        mean = x.mean(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.alpha * (x - mean) / (std + <span class="variable language_">self</span>.eps) + <span class="variable language_">self</span>.bias</span><br></pre></td></tr></table></figure>

<h3 id="前馈神经网络（Feed-Forward-Network）"><a href="#前馈神经网络（Feed-Forward-Network）" class="headerlink" title="前馈神经网络（Feed Forward Network）"></a>前馈神经网络（Feed Forward Network）</h3><p><strong>结构说明</strong></p>
<p>前馈网络由两个线性层组成，中间使用ReLU激活函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FFN(x) = ReLU(xW₁ + b₁)W₂ + b₂</span><br></pre></td></tr></table></figure>

<p><strong>维度变换</strong></p>
<ul>
<li>第一层：d_model → d_ff（扩展，通常d_ff &#x3D; 4 × d_model）</li>
<li>第二层：d_ff → d_model（压缩回原维度）</li>
</ul>
<p>论文中使用 d_model&#x3D;512，d_ff&#x3D;2048。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForwardBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;前馈神经网络块&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, d_ff: <span class="built_in">int</span>, dropout: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            d_model: 模型维度（输入输出维度）</span></span><br><span class="line"><span class="string">            d_ff: 中间层扩展维度（通常为d_model的4倍）</span></span><br><span class="line"><span class="string">            dropout: 随机失活概率</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.linear_2 = nn.Linear(d_ff, d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播: (batch, seq_len, d_model) → (batch, seq_len, d_ff) → (batch, seq_len, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linear_2(<span class="variable language_">self</span>.dropout(torch.relu(<span class="variable language_">self</span>.linear_1(x))))</span><br></pre></td></tr></table></figure>

<h3 id="多头注意力机制（Multi-Head-Attention）"><a href="#多头注意力机制（Multi-Head-Attention）" class="headerlink" title="多头注意力机制（Multi-Head Attention）"></a>多头注意力机制（Multi-Head Attention）</h3><p><strong>工作原理</strong></p>
<p>多头注意力机制将输入通过三个线性变换生成Q（查询）、K（键）、V（值）矩阵，然后：</p>
<ol>
<li><strong>线性投影</strong>：输入通过W_Q、W_K、W_V三个权重矩阵生成Q、K、V</li>
<li><strong>分头</strong>：将Q、K、V沿特征维度切分为h个头（如512维切分为8个64维的头）</li>
<li><strong>并行注意力</strong>：每个头独立计算缩放点积注意力</li>
<li><strong>合并</strong>：拼接所有头的输出，通过W_O线性层融合</li>
</ol>
<p><strong>核心流程</strong></p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/0510cc4123db8b450b53476b04afb66.png" alt="多头注意力流程图"></p>
<p><strong>分头策略</strong></p>
<ul>
<li>沿<strong>特征维度</strong>而非序列维度切分</li>
<li>每个头访问完整句子，但只看到嵌入的不同部分</li>
<li>例如：d_model&#x3D;512, h&#x3D;8 → 每个头的维度d_k&#x3D;64</li>
</ul>
<p><strong>关键特性</strong></p>
<ul>
<li>多头设计使模型同时关注不同语义关系（语法&#x2F;语义&#x2F;指代等）</li>
<li>输入输出维度一致（都是d_model），便于堆叠</li>
<li>编码器中Q&#x3D;K&#x3D;V（自注意力），解码器中K、V来自编码器（交叉注意力）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttentionBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;多头注意力机制（论文3.2.2节）&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, h: <span class="built_in">int</span>, dropout: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            d_model: 模型维度（必须能被h整除）</span></span><br><span class="line"><span class="string">            h: 注意力头数量</span></span><br><span class="line"><span class="string">            dropout: dropout概率</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model</span><br><span class="line">        <span class="variable language_">self</span>.h = h</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 确保d_model能被h整除</span></span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span>, <span class="string">&quot;d_model必须能被h整除&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.d_k = d_model // h  <span class="comment"># 每个头的维度</span></span><br><span class="line">       </span><br><span class="line">        <span class="comment"># 定义Q、K、V和输出的线性变换</span></span><br><span class="line">        <span class="variable language_">self</span>.w_q = nn.Linear(d_model, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.w_k = nn.Linear(d_model, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.w_v = nn.Linear(d_model, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.w_o = nn.Linear(d_model, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask, dropout: nn.Dropout</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        计算缩放点积注意力（论文3.2.1节）</span></span><br><span class="line"><span class="string">        返回: (注意力输出, 注意力权重)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        d_k = query.shape[-<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算注意力分数: Q·K^T / √d_k</span></span><br><span class="line">        attention_scores = (query @ key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 应用掩码（将无效位置设为极小值）</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention_scores.masked_fill_(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Softmax归一化</span></span><br><span class="line">        attention_scores = attention_scores.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 应用dropout</span></span><br><span class="line">        <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention_scores = dropout(attention_scores)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 注意力加权求和</span></span><br><span class="line">        <span class="keyword">return</span> (attention_scores @ value), attention_scores</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, q, k, v, mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播</span></span><br><span class="line"><span class="string">        输入/输出: (batch_size, seq_len, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 线性投影</span></span><br><span class="line">        query = <span class="variable language_">self</span>.w_q(q)  <span class="comment"># (batch, seq_len, d_model)</span></span><br><span class="line">        key = <span class="variable language_">self</span>.w_k(k)</span><br><span class="line">        value = <span class="variable language_">self</span>.w_v(v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 分头: (batch, seq_len, d_model) → (batch, h, seq_len, d_k)</span></span><br><span class="line">        query = query.view(query.shape[<span class="number">0</span>], query.shape[<span class="number">1</span>], <span class="variable language_">self</span>.h, <span class="variable language_">self</span>.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        key = key.view(key.shape[<span class="number">0</span>], key.shape[<span class="number">1</span>], <span class="variable language_">self</span>.h, <span class="variable language_">self</span>.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        value = value.view(value.shape[<span class="number">0</span>], value.shape[<span class="number">1</span>], <span class="variable language_">self</span>.h, <span class="variable language_">self</span>.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算注意力</span></span><br><span class="line">        x, <span class="variable language_">self</span>.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, <span class="variable language_">self</span>.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 合并头: (batch, h, seq_len, d_k) → (batch, seq_len, d_model)</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(x.shape[<span class="number">0</span>], -<span class="number">1</span>, <span class="variable language_">self</span>.h * <span class="variable language_">self</span>.d_k)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最终线性变换</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.w_o(x)</span><br></pre></td></tr></table></figure>

<p><strong>掩码机制说明</strong></p>
<p>掩码用于控制注意力的可见范围：</p>
<ul>
<li>在Softmax之前将不希望关注的位置设为-∞（实际使用-1e9）</li>
<li>Softmax后这些位置的权重接近0</li>
<li>用途：隐藏padding token、防止解码器看到未来信息</li>
</ul>
<h3 id="残差连接（Residual-Connection）"><a href="#残差连接（Residual-Connection）" class="headerlink" title="残差连接（Residual Connection）"></a>残差连接（Residual Connection）</h3><p><strong>功能说明</strong></p>
<p>残差连接（也称跳跃连接）将子层的输入直接加到输出上，缓解深层网络的梯度消失问题。</p>
<p><strong>实现细节</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输出 = LayerNorm(输入) → 子层 → Dropout → + 输入</span><br></pre></td></tr></table></figure>

<p>注意：这里采用Pre-LN结构（先归一化再计算），与原论文的Post-LN略有不同，但训练更稳定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualConnection</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;残差连接 + 层归一化 + Dropout&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features: <span class="built_in">int</span>, dropout: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.norm = LayerNormalization(features)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            x: 输入</span></span><br><span class="line"><span class="string">            sublayer: 子层函数（如注意力层或前馈层）</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> x + <span class="variable language_">self</span>.dropout(sublayer(<span class="variable language_">self</span>.norm(x)))</span><br></pre></td></tr></table></figure>

<h3 id="编码器块（Encoder-Block）"><a href="#编码器块（Encoder-Block）" class="headerlink" title="编码器块（Encoder Block）"></a>编码器块（Encoder Block）</h3><p><strong>结构组成</strong></p>
<p>每个编码器块包含两个子层：</p>
<ol>
<li>多头自注意力 + 残差连接 + 层归一化</li>
<li>前馈网络 + 残差连接 + 层归一化</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;单个编码器层&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features: <span class="built_in">int</span>, self_attention_block: MultiHeadAttentionBlock, </span></span><br><span class="line"><span class="params">                 feed_forward_block: FeedForwardBlock, dropout: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.self_attention_block = self_attention_block</span><br><span class="line">        <span class="variable language_">self</span>.feed_forward_block = feed_forward_block</span><br><span class="line">        <span class="variable language_">self</span>.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, src_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            x: 输入 (batch, seq_len, d_model)</span></span><br><span class="line"><span class="string">            src_mask: 源序列掩码（隐藏padding）</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 自注意力子层（Q=K=V=x）</span></span><br><span class="line">        x = <span class="variable language_">self</span>.residual_connections[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: <span class="variable language_">self</span>.self_attention_block(x, x, x, src_mask))</span><br><span class="line">        <span class="comment"># 前馈子层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.residual_connections[<span class="number">1</span>](x, <span class="variable language_">self</span>.feed_forward_block)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h3 id="完整编码器（Encoder）"><a href="#完整编码器（Encoder）" class="headerlink" title="完整编码器（Encoder）"></a>完整编码器（Encoder）</h3><p><strong>多层堆叠</strong></p>
<p>将N个编码器块堆叠，前一层的输出作为下一层的输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;完整编码器（N层编码器块堆叠）&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features: <span class="built_in">int</span>, layers: nn.ModuleList</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.layers = layers</span><br><span class="line">        <span class="variable language_">self</span>.norm = LayerNormalization(features)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;逐层处理输入&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.norm(x)</span><br></pre></td></tr></table></figure>

<h3 id="解码器块（Decoder-Block）"><a href="#解码器块（Decoder-Block）" class="headerlink" title="解码器块（Decoder Block）"></a>解码器块（Decoder Block）</h3><p><strong>结构组成</strong></p>
<p>解码器块包含三个子层（比编码器多一个交叉注意力层）：</p>
<ol>
<li><strong>掩码自注意力</strong>：Q&#x3D;K&#x3D;V来自解码器自身，使用目标掩码</li>
<li><strong>交叉注意力</strong>：Q来自解码器，K和V来自编码器输出</li>
<li><strong>前馈网络</strong>：与编码器相同的结构</li>
</ol>
<p><strong>自注意力 vs 交叉注意力</strong></p>
<ul>
<li><strong>自注意力</strong>：同一句子内的词相互关注（Q&#x3D;K&#x3D;V）</li>
<li><strong>交叉注意力</strong>：解码器关注编码器的输出（Q≠K,V）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;单个解码器层&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features: <span class="built_in">int</span>, self_attention_block: MultiHeadAttentionBlock, </span></span><br><span class="line"><span class="params">                 cross_attention_block: MultiHeadAttentionBlock, </span></span><br><span class="line"><span class="params">                 feed_forward_block: FeedForwardBlock, dropout: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.self_attention_block = self_attention_block</span><br><span class="line">        <span class="variable language_">self</span>.cross_attention_block = cross_attention_block</span><br><span class="line">        <span class="variable language_">self</span>.feed_forward_block = feed_forward_block</span><br><span class="line">        <span class="variable language_">self</span>.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, encoder_output, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            x: 解码器输入</span></span><br><span class="line"><span class="string">            encoder_output: 编码器输出</span></span><br><span class="line"><span class="string">            src_mask: 源序列掩码</span></span><br><span class="line"><span class="string">            tgt_mask: 目标序列掩码（因果掩码）</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 掩码自注意力（Q=K=V=x）</span></span><br><span class="line">        x = <span class="variable language_">self</span>.residual_connections[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: <span class="variable language_">self</span>.self_attention_block(x, x, x, tgt_mask))</span><br><span class="line">        <span class="comment"># 交叉注意力（Q=x, K=V=encoder_output）</span></span><br><span class="line">        x = <span class="variable language_">self</span>.residual_connections[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: <span class="variable language_">self</span>.cross_attention_block(x, encoder_output, encoder_output, src_mask))</span><br><span class="line">        <span class="comment"># 前馈网络</span></span><br><span class="line">        x = <span class="variable language_">self</span>.residual_connections[<span class="number">2</span>](x, <span class="variable language_">self</span>.feed_forward_block)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h3 id="完整解码器（Decoder）"><a href="#完整解码器（Decoder）" class="headerlink" title="完整解码器（Decoder）"></a>完整解码器（Decoder）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;完整解码器（N层解码器块堆叠）&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features: <span class="built_in">int</span>, layers: nn.ModuleList</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.layers = layers</span><br><span class="line">        <span class="variable language_">self</span>.norm = LayerNormalization(features)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, encoder_output, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;逐层处理输入&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            x = layer(x, encoder_output, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.norm(x)</span><br></pre></td></tr></table></figure>

<h3 id="投影层（Projection-Layer）"><a href="#投影层（Projection-Layer）" class="headerlink" title="投影层（Projection Layer）"></a>投影层（Projection Layer）</h3><p><strong>功能说明</strong></p>
<p>将解码器的输出（d_model维）投影到词汇表空间（vocab_size维），用于预测下一个token。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ProjectionLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;线性投影层：d_model → vocab_size&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab_size</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.proj = nn.Linear(d_model, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        输入: (batch, seq_len, d_model)</span></span><br><span class="line"><span class="string">        输出: (batch, seq_len, vocab_size)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.proj(x)</span><br></pre></td></tr></table></figure>

<h3 id="完整Transformer模型"><a href="#完整Transformer模型" class="headerlink" title="完整Transformer模型"></a>完整Transformer模型</h3><p><strong>模型组装</strong></p>
<p>将所有组件组装成完整的Transformer模型，包括编码器、解码器、嵌入层、位置编码和投影层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;完整的Transformer模型&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, </span></span><br><span class="line"><span class="params">                 tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, </span></span><br><span class="line"><span class="params">                 tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.encoder = encoder</span><br><span class="line">        <span class="variable language_">self</span>.decoder = decoder</span><br><span class="line">        <span class="variable language_">self</span>.src_embed = src_embed</span><br><span class="line">        <span class="variable language_">self</span>.tgt_embed = tgt_embed</span><br><span class="line">        <span class="variable language_">self</span>.src_pos = src_pos</span><br><span class="line">        <span class="variable language_">self</span>.tgt_pos = tgt_pos</span><br><span class="line">        <span class="variable language_">self</span>.projection_layer = projection_layer</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, src, src_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;编码源序列&quot;&quot;&quot;</span></span><br><span class="line">        src = <span class="variable language_">self</span>.src_embed(src)</span><br><span class="line">        src = <span class="variable language_">self</span>.src_pos(src)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.encoder(src, src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, encoder_output: torch.Tensor, src_mask: torch.Tensor, </span></span><br><span class="line"><span class="params">               tgt: torch.Tensor, tgt_mask: torch.Tensor</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;解码目标序列&quot;&quot;&quot;</span></span><br><span class="line">        tgt = <span class="variable language_">self</span>.tgt_embed(tgt)</span><br><span class="line">        tgt = <span class="variable language_">self</span>.tgt_pos(tgt)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.decoder(tgt, encoder_output, src_mask, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">project</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;投影到词汇表空间&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.projection_layer(x)</span><br></pre></td></tr></table></figure>

<h3 id="模型构建函数"><a href="#模型构建函数" class="headerlink" title="模型构建函数"></a>模型构建函数</h3><p><strong>功能说明</strong></p>
<p>根据超参数构建完整的Transformer模型，并使用Xavier初始化参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_transformer</span>(<span class="params">src_vocab_size: <span class="built_in">int</span>, tgt_vocab_size: <span class="built_in">int</span>, src_seq_len: <span class="built_in">int</span>, </span></span><br><span class="line"><span class="params">                     tgt_seq_len: <span class="built_in">int</span>, d_model: <span class="built_in">int</span>=<span class="number">512</span>, N: <span class="built_in">int</span>=<span class="number">6</span>, h: <span class="built_in">int</span>=<span class="number">8</span>, </span></span><br><span class="line"><span class="params">                     dropout: <span class="built_in">float</span>=<span class="number">0.1</span>, d_ff: <span class="built_in">int</span>=<span class="number">2048</span></span>) -&gt; Transformer:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    构建Transformer模型</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        src_vocab_size: 源语言词汇表大小</span></span><br><span class="line"><span class="string">        tgt_vocab_size: 目标语言词汇表大小</span></span><br><span class="line"><span class="string">        src_seq_len: 源序列最大长度</span></span><br><span class="line"><span class="string">        tgt_seq_len: 目标序列最大长度</span></span><br><span class="line"><span class="string">        d_model: 模型维度（默认512）</span></span><br><span class="line"><span class="string">        N: 编码器/解码器层数（默认6）</span></span><br><span class="line"><span class="string">        h: 注意力头数（默认8）</span></span><br><span class="line"><span class="string">        dropout: dropout概率（默认0.1）</span></span><br><span class="line"><span class="string">        d_ff: 前馈网络中间层维度（默认2048）</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建嵌入层</span></span><br><span class="line">    src_embed = InputEmbeddings(d_model, src_vocab_size)</span><br><span class="line">    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建位置编码层</span></span><br><span class="line">    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)</span><br><span class="line">    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建编码器块</span></span><br><span class="line">    encoder_blocks = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)</span><br><span class="line">        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)</span><br><span class="line">        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)</span><br><span class="line">        encoder_blocks.append(encoder_block)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建解码器块</span></span><br><span class="line">    decoder_blocks = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)</span><br><span class="line">        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)</span><br><span class="line">        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)</span><br><span class="line">        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)</span><br><span class="line">        decoder_blocks.append(decoder_block)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建编码器和解码器</span></span><br><span class="line">    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))</span><br><span class="line">    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建投影层</span></span><br><span class="line">    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 组装Transformer</span></span><br><span class="line">    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用Xavier均匀初始化参数</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> transformer.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform_(p)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> transformer</span><br></pre></td></tr></table></figure>

<h2 id="Transformer训练实现"><a href="#Transformer训练实现" class="headerlink" title="Transformer训练实现"></a>Transformer训练实现</h2><p>本节介绍如何训练Transformer模型，包括数据加载、训练循环、验证和推理。</p>
<h3 id="导入依赖库"><a href="#导入依赖库" class="headerlink" title="导入依赖库"></a>导入依赖库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义模块</span></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> build_transformer</span><br><span class="line"><span class="keyword">from</span> dataset <span class="keyword">import</span> BilingualDataset, causal_mask</span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> get_config, get_weights_file_path, latest_weights_file_path</span><br><span class="line"></span><br><span class="line"><span class="comment"># PyTorch核心库</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, random_split</span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> LambdaLR</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据处理</span></span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">from</span> tokenizers.models <span class="keyword">import</span> WordLevel</span><br><span class="line"><span class="keyword">from</span> tokenizers.trainers <span class="keyword">import</span> WordLevelTrainer</span><br><span class="line"><span class="keyword">from</span> tokenizers.pre_tokenizers <span class="keyword">import</span> Whitespace</span><br><span class="line"></span><br><span class="line"><span class="comment"># 工具库</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> torchmetrics</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br></pre></td></tr></table></figure>

<h3 id="贪心解码函数"><a href="#贪心解码函数" class="headerlink" title="贪心解码函数"></a>贪心解码函数</h3><p><strong>功能说明</strong></p>
<p>在推理阶段使用贪心算法逐个生成目标序列的token，每次选择概率最高的token。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">greedy_decode</span>(<span class="params">model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    贪心解码：逐token生成目标序列</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        model: 训练好的Transformer模型</span></span><br><span class="line"><span class="string">        source: 源序列 (已编码)</span></span><br><span class="line"><span class="string">        source_mask: 源序列掩码</span></span><br><span class="line"><span class="string">        tokenizer_src/tgt: 源/目标语言分词器</span></span><br><span class="line"><span class="string">        max_len: 生成序列最大长度</span></span><br><span class="line"><span class="string">        device: 计算设备</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">        生成的目标序列 (token IDs)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    sos_idx = tokenizer_tgt.token_to_id(<span class="string">&#x27;[SOS]&#x27;</span>)</span><br><span class="line">    eos_idx = tokenizer_tgt.token_to_id(<span class="string">&#x27;[EOS]&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预计算编码器输出（只需计算一次）</span></span><br><span class="line">    encoder_output = model.encode(source, source_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化解码器输入为[SOS]</span></span><br><span class="line">    decoder_input = torch.empty(<span class="number">1</span>, <span class="number">1</span>).fill_(sos_idx).type_as(source).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 自回归生成</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">if</span> decoder_input.size(<span class="number">1</span>) == max_len:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建因果掩码</span></span><br><span class="line">        decoder_mask = causal_mask(decoder_input.size(<span class="number">1</span>)).type_as(source_mask).to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 解码</span></span><br><span class="line">        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 投影到词汇表并选择概率最高的token</span></span><br><span class="line">        prob = model.project(out[:, -<span class="number">1</span>])</span><br><span class="line">        _, next_word = torch.<span class="built_in">max</span>(prob, dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将新token添加到序列</span></span><br><span class="line">        decoder_input = torch.cat([</span><br><span class="line">            decoder_input,</span><br><span class="line">            torch.empty(<span class="number">1</span>, <span class="number">1</span>).type_as(source).fill_(next_word.item()).to(device)</span><br><span class="line">        ], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 遇到EOS则停止</span></span><br><span class="line">        <span class="keyword">if</span> next_word == eos_idx:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> decoder_input.squeeze(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h3 id="验证函数"><a href="#验证函数" class="headerlink" title="验证函数"></a>验证函数</h3><p><strong>功能说明</strong></p>
<p>在验证集上评估模型性能，计算CER、WER和BLEU等指标。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">run_validation</span>(<span class="params">model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, </span></span><br><span class="line"><span class="params">                  print_msg, global_step, writer, num_examples=<span class="number">2</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    运行模型验证</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        model: Transformer模型</span></span><br><span class="line"><span class="string">        validation_ds: 验证数据集</span></span><br><span class="line"><span class="string">        tokenizer_src/tgt: 分词器</span></span><br><span class="line"><span class="string">        max_len: 最大序列长度</span></span><br><span class="line"><span class="string">        device: 计算设备</span></span><br><span class="line"><span class="string">        print_msg: 打印函数</span></span><br><span class="line"><span class="string">        global_step: 当前训练步数</span></span><br><span class="line"><span class="string">        writer: TensorBoard写入器</span></span><br><span class="line"><span class="string">        num_examples: 验证样本数量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    source_texts = []</span><br><span class="line">    expected = []</span><br><span class="line">    predicted = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">with</span> os.popen(<span class="string">&#x27;stty size&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> console:</span><br><span class="line">            _, console_width = console.read().split()</span><br><span class="line">            console_width = <span class="built_in">int</span>(console_width)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        console_width = <span class="number">80</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> validation_ds:</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">            encoder_input = batch[<span class="string">&quot;encoder_input&quot;</span>].to(device)</span><br><span class="line">            encoder_mask = batch[<span class="string">&quot;encoder_mask&quot;</span>].to(device)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">assert</span> encoder_input.size(<span class="number">0</span>) == <span class="number">1</span>, <span class="string">&quot;验证时batch_size必须为1&quot;</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 贪心解码生成预测</span></span><br><span class="line">            model_out = greedy_decode(model, encoder_input, encoder_mask, </span><br><span class="line">                                     tokenizer_src, tokenizer_tgt, max_len, device)</span><br><span class="line"></span><br><span class="line">            source_text = batch[<span class="string">&quot;src_text&quot;</span>][<span class="number">0</span>]</span><br><span class="line">            target_text = batch[<span class="string">&quot;tgt_text&quot;</span>][<span class="number">0</span>]</span><br><span class="line">            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())</span><br><span class="line"></span><br><span class="line">            source_texts.append(source_text)</span><br><span class="line">            expected.append(target_text)</span><br><span class="line">            predicted.append(model_out_text)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 打印示例</span></span><br><span class="line">            print_msg(<span class="string">&#x27;-&#x27;</span> * console_width)</span><br><span class="line">            print_msg(<span class="string">f&quot;<span class="subst">&#123;<span class="string">f&#x27;SOURCE: &#x27;</span>:&gt;<span class="number">12</span>&#125;</span><span class="subst">&#123;source_text&#125;</span>&quot;</span>)</span><br><span class="line">            print_msg(<span class="string">f&quot;<span class="subst">&#123;<span class="string">f&#x27;TARGET: &#x27;</span>:&gt;<span class="number">12</span>&#125;</span><span class="subst">&#123;target_text&#125;</span>&quot;</span>)</span><br><span class="line">            print_msg(<span class="string">f&quot;<span class="subst">&#123;<span class="string">f&#x27;PREDICTED: &#x27;</span>:&gt;<span class="number">12</span>&#125;</span><span class="subst">&#123;model_out_text&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> count == num_examples:</span><br><span class="line">                print_msg(<span class="string">&#x27;-&#x27;</span> * console_width)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算评估指标</span></span><br><span class="line">    <span class="keyword">if</span> writer:</span><br><span class="line">        <span class="comment"># 字符错误率</span></span><br><span class="line">        metric = torchmetrics.CharErrorRate()</span><br><span class="line">        cer = metric(predicted, expected)</span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;validation cer&#x27;</span>, cer, global_step)</span><br><span class="line">        writer.flush()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 词错误率</span></span><br><span class="line">        metric = torchmetrics.WordErrorRate()</span><br><span class="line">        wer = metric(predicted, expected)</span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;validation wer&#x27;</span>, wer, global_step)</span><br><span class="line">        writer.flush()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># BLEU分数</span></span><br><span class="line">        metric = torchmetrics.BLEUScore()</span><br><span class="line">        bleu = metric(predicted, expected)</span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;validation BLEU&#x27;</span>, bleu, global_step)</span><br><span class="line">        writer.flush()</span><br></pre></td></tr></table></figure>

<h3 id="数据处理函数"><a href="#数据处理函数" class="headerlink" title="数据处理函数"></a>数据处理函数</h3><p><strong>获取句子生成器</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_all_sentences</span>(<span class="params">ds, lang</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    从数据集提取指定语言的所有句子（生成器）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        ds: 数据集对象</span></span><br><span class="line"><span class="string">        lang: 语言代码（如&#x27;en&#x27;, &#x27;fr&#x27;）</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> ds:</span><br><span class="line">        <span class="keyword">yield</span> item[<span class="string">&#x27;translation&#x27;</span>][lang]</span><br></pre></td></tr></table></figure>

<p><strong>构建或加载分词器</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_or_build_tokenizer</span>(<span class="params">config, ds, lang</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    获取或构建指定语言的分词器</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        config: 配置字典</span></span><br><span class="line"><span class="string">        ds: 数据集（用于训练分词器）</span></span><br><span class="line"><span class="string">        lang: 语言代码</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">        Tokenizer对象</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    tokenizer_path = Path(config[<span class="string">&#x27;tokenizer_file&#x27;</span>].<span class="built_in">format</span>(lang))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> Path.exists(tokenizer_path):</span><br><span class="line">        <span class="comment"># 训练新分词器</span></span><br><span class="line">        tokenizer = Tokenizer(WordLevel(unk_token=<span class="string">&quot;[UNK]&quot;</span>))</span><br><span class="line">        tokenizer.pre_tokenizer = Whitespace()</span><br><span class="line">        trainer = WordLevelTrainer(</span><br><span class="line">            special_tokens=[<span class="string">&quot;[UNK]&quot;</span>, <span class="string">&quot;[PAD]&quot;</span>, <span class="string">&quot;[SOS]&quot;</span>, <span class="string">&quot;[EOS]&quot;</span>], </span><br><span class="line">            min_frequency=<span class="number">2</span></span><br><span class="line">        )</span><br><span class="line">        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)</span><br><span class="line">        tokenizer.save(<span class="built_in">str</span>(tokenizer_path))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 加载已有分词器</span></span><br><span class="line">        tokenizer = Tokenizer.from_file(<span class="built_in">str</span>(tokenizer_path))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tokenizer</span><br></pre></td></tr></table></figure>

<p><strong>加载并准备数据集</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_ds</span>(<span class="params">config</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    加载数据集并创建数据加载器</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">        train_dataloader: 训练集加载器</span></span><br><span class="line"><span class="string">        val_dataloader: 验证集加载器</span></span><br><span class="line"><span class="string">        tokenizer_src: 源语言分词器</span></span><br><span class="line"><span class="string">        tokenizer_tgt: 目标语言分词器</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 加载原始数据集</span></span><br><span class="line">    ds_raw = load_dataset(</span><br><span class="line">        <span class="string">f&quot;<span class="subst">&#123;config[<span class="string">&#x27;datasource&#x27;</span>]&#125;</span>&quot;</span>,</span><br><span class="line">        <span class="string">f&quot;<span class="subst">&#123;config[<span class="string">&#x27;lang_src&#x27;</span>]&#125;</span>-<span class="subst">&#123;config[<span class="string">&#x27;lang_tgt&#x27;</span>]&#125;</span>&quot;</span>,</span><br><span class="line">        split=<span class="string">&#x27;train&#x27;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建分词器</span></span><br><span class="line">    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config[<span class="string">&#x27;lang_src&#x27;</span>])</span><br><span class="line">    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config[<span class="string">&#x27;lang_tgt&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 划分训练集和验证集（90%/10%）</span></span><br><span class="line">    train_ds_size = <span class="built_in">int</span>(<span class="number">0.9</span> * <span class="built_in">len</span>(ds_raw))</span><br><span class="line">    val_ds_size = <span class="built_in">len</span>(ds_raw) - train_ds_size</span><br><span class="line">    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建BilingualDataset实例</span></span><br><span class="line">    train_ds = BilingualDataset(</span><br><span class="line">        train_ds_raw, tokenizer_src, tokenizer_tgt,</span><br><span class="line">        config[<span class="string">&#x27;lang_src&#x27;</span>], config[<span class="string">&#x27;lang_tgt&#x27;</span>], config[<span class="string">&#x27;seq_len&#x27;</span>]</span><br><span class="line">    )</span><br><span class="line">    val_ds = BilingualDataset(</span><br><span class="line">        val_ds_raw, tokenizer_src, tokenizer_tgt,</span><br><span class="line">        config[<span class="string">&#x27;lang_src&#x27;</span>], config[<span class="string">&#x27;lang_tgt&#x27;</span>], config[<span class="string">&#x27;seq_len&#x27;</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 统计最大句子长度</span></span><br><span class="line">    max_len_src = <span class="number">0</span></span><br><span class="line">    max_len_tgt = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> ds_raw:</span><br><span class="line">        src_ids = tokenizer_src.encode(item[<span class="string">&#x27;translation&#x27;</span>][config[<span class="string">&#x27;lang_src&#x27;</span>]]).ids</span><br><span class="line">        tgt_ids = tokenizer_tgt.encode(item[<span class="string">&#x27;translation&#x27;</span>][config[<span class="string">&#x27;lang_tgt&#x27;</span>]]).ids</span><br><span class="line">        max_len_src = <span class="built_in">max</span>(max_len_src, <span class="built_in">len</span>(src_ids))</span><br><span class="line">        max_len_tgt = <span class="built_in">max</span>(max_len_tgt, <span class="built_in">len</span>(tgt_ids))</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;源语言最大长度: <span class="subst">&#123;max_len_src&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;目标语言最大长度: <span class="subst">&#123;max_len_tgt&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建数据加载器</span></span><br><span class="line">    train_dataloader = DataLoader(train_ds, batch_size=config[<span class="string">&#x27;batch_size&#x27;</span>], shuffle=<span class="literal">True</span>)</span><br><span class="line">    val_dataloader = DataLoader(val_ds, batch_size=<span class="number">1</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt</span><br></pre></td></tr></table></figure>

<p><strong>构建模型</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_model</span>(<span class="params">config, vocab_src_len, vocab_tgt_len</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    构建Transformer模型</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        config: 配置字典</span></span><br><span class="line"><span class="string">        vocab_src_len: 源语言词汇表大小</span></span><br><span class="line"><span class="string">        vocab_tgt_len: 目标语言词汇表大小</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    model = build_transformer(</span><br><span class="line">        vocab_src_len, vocab_tgt_len,</span><br><span class="line">        config[<span class="string">&quot;seq_len&quot;</span>], config[<span class="string">&#x27;seq_len&#x27;</span>],</span><br><span class="line">        d_model=config[<span class="string">&#x27;d_model&#x27;</span>]</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<h3 id="训练主函数"><a href="#训练主函数" class="headerlink" title="训练主函数"></a>训练主函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">config</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Transformer模型训练主函数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        config: 配置字典，包含所有训练超参数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 选择设备</span></span><br><span class="line">    device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;mps&quot;</span> <span class="keyword">if</span> torch.has_mps <span class="keyword">or</span> torch.backends.mps.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;使用设备:&quot;</span>, device)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> device == <span class="string">&#x27;cuda&#x27;</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;设备名称: <span class="subst">&#123;torch.cuda.get_device_name(device.index)&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;显存大小: <span class="subst">&#123;torch.cuda.get_device_properties(device.index).total_memory / <span class="number">1024</span> ** <span class="number">3</span>&#125;</span> GB&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    device = torch.device(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建权重保存目录</span></span><br><span class="line">    Path(<span class="string">f&quot;<span class="subst">&#123;config[<span class="string">&#x27;datasource&#x27;</span>]&#125;</span>_<span class="subst">&#123;config[<span class="string">&#x27;model_folder&#x27;</span>]&#125;</span>&quot;</span>).mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载数据集</span></span><br><span class="line">    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建模型</span></span><br><span class="line">    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># TensorBoard记录器</span></span><br><span class="line">    writer = SummaryWriter(config[<span class="string">&#x27;experiment_name&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 优化器</span></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=config[<span class="string">&#x27;lr&#x27;</span>], eps=<span class="number">1e-9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载预训练模型（如果有）</span></span><br><span class="line">    initial_epoch = <span class="number">0</span></span><br><span class="line">    global_step = <span class="number">0</span></span><br><span class="line">    preload = config[<span class="string">&#x27;preload&#x27;</span>]</span><br><span class="line">    model_filename = latest_weights_file_path(config) <span class="keyword">if</span> preload == <span class="string">&#x27;latest&#x27;</span> <span class="keyword">else</span> get_weights_file_path(config, preload) <span class="keyword">if</span> preload <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> model_filename:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;加载模型: <span class="subst">&#123;model_filename&#125;</span>&#x27;</span>)</span><br><span class="line">        state = torch.load(model_filename)</span><br><span class="line">        model.load_state_dict(state[<span class="string">&#x27;model_state_dict&#x27;</span>])</span><br><span class="line">        initial_epoch = state[<span class="string">&#x27;epoch&#x27;</span>] + <span class="number">1</span></span><br><span class="line">        optimizer.load_state_dict(state[<span class="string">&#x27;optimizer_state_dict&#x27;</span>])</span><br><span class="line">        global_step = state[<span class="string">&#x27;global_step&#x27;</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;从头开始训练&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 损失函数（带标签平滑）</span></span><br><span class="line">    loss_fn = nn.CrossEntropyLoss(</span><br><span class="line">        ignore_index=tokenizer_src.token_to_id(<span class="string">&#x27;[PAD]&#x27;</span>),</span><br><span class="line">        label_smoothing=<span class="number">0.1</span></span><br><span class="line">    ).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练循环</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(initial_epoch, config[<span class="string">&#x27;num_epochs&#x27;</span>]):</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line">        model.train()</span><br><span class="line">        batch_iterator = tqdm(train_dataloader, desc=<span class="string">f&quot;Epoch <span class="subst">&#123;epoch:02d&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> batch_iterator:</span><br><span class="line">            <span class="comment"># 准备数据</span></span><br><span class="line">            encoder_input = batch[<span class="string">&#x27;encoder_input&#x27;</span>].to(device)</span><br><span class="line">            decoder_input = batch[<span class="string">&#x27;decoder_input&#x27;</span>].to(device)</span><br><span class="line">            encoder_mask = batch[<span class="string">&#x27;encoder_mask&#x27;</span>].to(device)</span><br><span class="line">            decoder_mask = batch[<span class="string">&#x27;decoder_mask&#x27;</span>].to(device)</span><br><span class="line">            label = batch[<span class="string">&#x27;label&#x27;</span>].to(device)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 前向传播</span></span><br><span class="line">            encoder_output = model.encode(encoder_input, encoder_mask)</span><br><span class="line">            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)</span><br><span class="line">            proj_output = model.project(decoder_output)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算损失</span></span><br><span class="line">            loss = loss_fn(proj_output.view(-<span class="number">1</span>, tokenizer_tgt.get_vocab_size()), label.view(-<span class="number">1</span>))</span><br><span class="line">            batch_iterator.set_postfix(&#123;<span class="string">&quot;loss&quot;</span>: <span class="string">f&quot;<span class="subst">&#123;loss.item():<span class="number">6.3</span>f&#125;</span>&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 记录到TensorBoard</span></span><br><span class="line">            writer.add_scalar(<span class="string">&#x27;train loss&#x27;</span>, loss.item(), global_step)</span><br><span class="line">            writer.flush()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 反向传播</span></span><br><span class="line">            loss.backward()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 更新参数</span></span><br><span class="line">            optimizer.step()</span><br><span class="line">            optimizer.zero_grad(set_to_none=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">            global_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每个epoch后运行验证</span></span><br><span class="line">        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, </span><br><span class="line">                      config[<span class="string">&#x27;seq_len&#x27;</span>], device, <span class="keyword">lambda</span> msg: batch_iterator.write(msg), </span><br><span class="line">                      global_step, writer)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 保存模型</span></span><br><span class="line">        model_filename = get_weights_file_path(config, <span class="string">f&quot;<span class="subst">&#123;epoch:02d&#125;</span>&quot;</span>)</span><br><span class="line">        torch.save(&#123;</span><br><span class="line">            <span class="string">&#x27;epoch&#x27;</span>: epoch,</span><br><span class="line">            <span class="string">&#x27;model_state_dict&#x27;</span>: model.state_dict(),</span><br><span class="line">            <span class="string">&#x27;optimizer_state_dict&#x27;</span>: optimizer.state_dict(),</span><br><span class="line">            <span class="string">&#x27;global_step&#x27;</span>: global_step</span><br><span class="line">        &#125;, model_filename)</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文详细介绍了Transformer模型的原理和PyTorch实现，主要内容包括：</p>
<p><strong>核心组件</strong></p>
<ul>
<li>输入嵌入与位置编码：将离散token转换为连续向量并注入位置信息</li>
<li>多头注意力机制：从多个角度并行捕捉序列中的依赖关系</li>
<li>前馈神经网络：对每个位置独立进行非线性变换</li>
<li>层归一化与残差连接：稳定训练过程，缓解梯度消失</li>
</ul>
<p><strong>模型架构</strong></p>
<ul>
<li>编码器：6层堆叠，每层包含自注意力和前馈网络</li>
<li>解码器：6层堆叠，每层包含自注意力、交叉注意力和前馈网络</li>
<li>投影层：将解码器输出映射到词汇表空间</li>
</ul>
<p><strong>训练流程</strong></p>
<ul>
<li>数据处理：分词、批处理、掩码生成</li>
<li>训练循环：前向传播、损失计算、反向传播、参数更新</li>
<li>验证评估：使用CER、WER、BLEU等指标评估模型性能</li>
</ul>
<p>Transformer的成功在于其完全基于注意力机制的架构设计，摒弃了传统的循环结构，实现了高效的并行计算和长距离依赖建模，为后续的BERT、GPT等预训练模型奠定了基础。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    # 清空梯度，节省内存</span><br><span class="line">    optimizer.zero_grad(set_to_none=True)</span><br><span class="line"></span><br><span class="line">    # 更新步数</span><br><span class="line">    global_step += 1</span><br><span class="line"></span><br><span class="line"># 每个epoch结束后运行验证逻辑</span><br><span class="line">run_validation(</span><br><span class="line">    model,</span><br><span class="line">    val_dataloader,</span><br><span class="line">    tokenizer_src,</span><br><span class="line">    tokenizer_tgt,</span><br><span class="line">    config[&#x27;seq_len&#x27;],</span><br><span class="line">    device,</span><br><span class="line">    lambda msg: batch_iterator.write(msg),  # 用 tqdm 输出日志信息</span><br><span class="line">    global_step,</span><br><span class="line">    writer</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 每个epoch结束后保存模型状态</span><br><span class="line">model_filename = get_weights_file_path(config, f&quot;&#123;epoch:02d&#125;&quot;)</span><br><span class="line">torch.save(&#123;</span><br><span class="line">    &#x27;epoch&#x27;: epoch,</span><br><span class="line">    &#x27;model_state_dict&#x27;: model.state_dict(),</span><br><span class="line">    &#x27;optimizer_state_dict&#x27;: optimizer.state_dict(),</span><br><span class="line">    &#x27;global_step&#x27;: global_step</span><br><span class="line">&#125;, model_filename)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:  <span class="comment"># 主程序入口，确保当前脚本被直接运行时才执行以下代码</span></span><br><span class="line">    warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)  <span class="comment"># 忽略所有警告信息，保持输出界面清爽</span></span><br><span class="line">    config = get_config()  <span class="comment"># 获取训练配置参数，通常来自配置文件或定义函数</span></span><br><span class="line">    train_model(config)  <span class="comment"># 调用训练函数，开始模型训练流程</span></span><br></pre></td></tr></table></figure>

<h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#处理双语数据集</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br></pre></td></tr></table></figure>

<p><strong>import torch</strong>：导入 PyTorch 主库，用于张量运算。</p>
<p><strong>import torch.nn as nn</strong>：导入神经网络模块并简写为 <code>nn</code>，方便后续如果需要网络层时使用。</p>
<p><strong>from torch.utils.data import Dataset</strong>：从PyTorch数据工具中导入 <code>Dataset</code> 基类，用来构建自定义数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义一个新的数据集类 BilingualDataset，继承自 PyTorch 的 Dataset，用于加载双语翻译对。</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BilingualDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.seq_len = seq_len</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.ds = ds</span><br><span class="line">        <span class="variable language_">self</span>.tokenizer_src = tokenizer_src</span><br><span class="line">        <span class="variable language_">self</span>.tokenizer_tgt = tokenizer_tgt</span><br><span class="line">        <span class="variable language_">self</span>.src_lang = src_lang</span><br><span class="line">        <span class="variable language_">self</span>.tgt_lang = tgt_lang</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>init</strong>：初始化方法，接受以下参数：</p>
<ul>
<li><code>ds</code>：原始数据集（如从 HuggingFace Dataset 加载的翻译对）。</li>
<li><code>tokenizer_src</code> &#x2F; <code>tokenizer_tgt</code>：源语言和目标语言的分词器。</li>
<li><code>src_lang</code> &#x2F; <code>tgt_lang</code>：字符串，指明在每个数据项里使用哪个语言字段。</li>
<li><code>seq_len</code>：固定的序列长度（包含特殊token）。</li>
<li>将这些参数<strong>保存</strong>到实例属性，以便后续 <code>__getitem__</code> 中使用。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.sos_token = torch.tensor([tokenizer_tgt.token_to_id(<span class="string">&quot;[SOS]&quot;</span>)], dtype=torch.int64)</span><br><span class="line"><span class="variable language_">self</span>.eos_token = torch.tensor([tokenizer_tgt.token_to_id(<span class="string">&quot;[EOS]&quot;</span>)], dtype=torch.int64)</span><br><span class="line"><span class="variable language_">self</span>.pad_token = torch.tensor([tokenizer_tgt.token_to_id(<span class="string">&quot;[PAD]&quot;</span>)], dtype=torch.int64)</span><br></pre></td></tr></table></figure>

<p>从目标分词器中获取特殊符号 <code>[SOS]</code>、<code>[EOS]</code>、<code>[PAD]</code> 的ID，并封装成形状为 <code>(1,)</code> 的整型张量，方便后面拼接。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义数据集长度</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.ds)</span><br></pre></td></tr></table></figure>

<p><strong>len</strong>：返回数据集的条目数，使得 DataLoader 能够知道迭代上限。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义获取项目方法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        src_target_pair = <span class="variable language_">self</span>.ds[idx]</span><br><span class="line">        src_text = src_target_pair[<span class="string">&#x27;translation&#x27;</span>][<span class="variable language_">self</span>.src_lang]</span><br><span class="line">        tgt_text = src_target_pair[<span class="string">&#x27;translation&#x27;</span>][<span class="variable language_">self</span>.tgt_lang]</span><br></pre></td></tr></table></figure>

<p><strong>getitem</strong>：根据索引 <code>idx</code> 取出一条翻译对，分别抽取源语言文本 <code>src_text</code> 和目标语言文本 <code>tgt_text</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Transform the text into tokens</span></span><br><span class="line">enc_input_tokens = <span class="variable language_">self</span>.tokenizer_src.encode(src_text).ids</span><br><span class="line">dec_input_tokens = <span class="variable language_">self</span>.tokenizer_tgt.encode(tgt_text).ids</span><br></pre></td></tr></table></figure>

<p>分别对源文和目标文进行分词，得到 ID 列表 <code>enc_input_tokens</code>、<code>dec_input_tokens</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add sos, eos and padding to each sentence</span></span><br><span class="line">enc_num_padding_tokens = <span class="variable language_">self</span>.seq_len - <span class="built_in">len</span>(enc_input_tokens) - <span class="number">2</span>  <span class="comment"># We will add &lt;s&gt; and &lt;/s&gt;</span></span><br><span class="line"><span class="comment"># We will only add &lt;s&gt;, and &lt;/s&gt; only on the label</span></span><br><span class="line">dec_num_padding_tokens = <span class="variable language_">self</span>.seq_len - <span class="built_in">len</span>(dec_input_tokens) - <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>计算要补多少个 <code>[PAD]</code>：</p>
<ul>
<li>源端要加 <code>[SOS]</code>+<code>[EOS]</code> 共2个特殊符，故剩余长度为 <code>seq_len - 原始长度 - 2</code>。</li>
<li>目标端的 decoder 输入只加 <code>[SOS]</code>，故剩余长度为 <code>seq_len - 原始长度 - 1</code>。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> enc_num_padding_tokens &lt; <span class="number">0</span> <span class="keyword">or</span> dec_num_padding_tokens &lt; <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">&quot;Sentence is too long&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>如果算出来的补齐长度<strong>为负</strong>，说明句子太长，超过了 <code>seq_len</code>，直接抛错提醒。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#切割三个张量</span></span><br><span class="line"><span class="comment"># Add &lt;s&gt; and &lt;/s&gt; token</span></span><br><span class="line">        encoder_input = torch.cat(</span><br><span class="line">            [</span><br><span class="line">                <span class="variable language_">self</span>.sos_token,<span class="comment">#首先是这个句子的开头标记，</span></span><br><span class="line">                torch.tensor(enc_input_tokens, dtype=torch.int64),<span class="comment">#然后是源文本的标记</span></span><br><span class="line">                <span class="variable language_">self</span>.eos_token,<span class="comment">#然后是句子的结尾标记 </span></span><br><span class="line">                torch.tensor([<span class="variable language_">self</span>.pad_token] * enc_num_padding_tokens, dtype=torch.int64),<span class="comment">#然后是足够的填充标记以达到序列长度</span></span><br><span class="line">            ],</span><br><span class="line">            dim=<span class="number">0</span>,</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<p>构造 Encoder 输入序列：</p>
<ol>
<li><p>添加 <code>[SOS]</code>；</p>
</li>
<li><p>源语言分词 ID；</p>
</li>
<li><p>添加 <code>[EOS]</code>；</p>
</li>
<li><p>补齐若干个 <code>[PAD]</code>；</p>
<p> 最后拼成形状 <code>(seq_len,)</code> 的张量。</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add only &lt;s&gt; token</span></span><br><span class="line">decoder_input = torch.cat(</span><br><span class="line">    [</span><br><span class="line">        <span class="variable language_">self</span>.sos_token,</span><br><span class="line">        torch.tensor(dec_input_tokens, dtype=torch.int64),</span><br><span class="line">        torch.tensor([<span class="variable language_">self</span>.pad_token] * dec_num_padding_tokens, dtype=torch.int64),</span><br><span class="line">    ],</span><br><span class="line">    dim=<span class="number">0</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>构造Decoder输入序列：只在最前面加 <code>[SOS]</code> 和尾部补齐 <code>[PAD]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add only &lt;/s&gt; token</span></span><br><span class="line">label = torch.cat(</span><br><span class="line">    [</span><br><span class="line">        torch.tensor(dec_input_tokens, dtype=torch.int64),</span><br><span class="line">        <span class="variable language_">self</span>.eos_token,</span><br><span class="line">        torch.tensor([<span class="variable language_">self</span>.pad_token] * dec_num_padding_tokens, dtype=torch.int64),</span><br><span class="line">    ],</span><br><span class="line">    dim=<span class="number">0</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>构造训练目标（标签）序列：紧跟分词 ID 后面加 <code>[EOS]</code>，再补齐。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Double check the size of the tensors to make sure they are all seq_len long</span></span><br><span class="line"><span class="keyword">assert</span> encoder_input.size(<span class="number">0</span>) == <span class="variable language_">self</span>.seq_len</span><br><span class="line"><span class="keyword">assert</span> decoder_input.size(<span class="number">0</span>) == <span class="variable language_">self</span>.seq_len</span><br><span class="line"><span class="keyword">assert</span> label.size(<span class="number">0</span>) == <span class="variable language_">self</span>.seq_len</span><br></pre></td></tr></table></figure>

<p>断言三者长度都等于 <code>seq_len</code>，<strong>保证模型输入输出的一致性。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> &#123;</span><br><span class="line">    <span class="string">&quot;encoder_input&quot;</span>: encoder_input,  <span class="comment"># (seq_len)编码器输入,是越位的序列长度</span></span><br><span class="line">    <span class="string">&quot;decoder_input&quot;</span>: decoder_input,  <span class="comment"># (seq_len)解码器输入,是一个序列长度的标记数</span></span><br><span class="line">    <span class="string">&quot;encoder_mask&quot;</span>: (encoder_input != <span class="variable language_">self</span>.pad_token).unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>).<span class="built_in">int</span>(), <span class="comment"># (1, 1, seq_len)</span></span><br><span class="line">    <span class="comment">#编码器掩码，通过添加填充标记来增加编码器输入句子的大小，但是我们 我们不希望这些填充标记参与自注意力机制，所以我们需要构建一个掩码，不希望这些标记被自注意力机制看到。我们还会挤压以添加此序列维度，稍后还会添加批处理维度。然后我们将其转换为整数，因此这是一个序列长度。</span></span><br><span class="line">    <span class="string">&quot;decoder_mask&quot;</span>: (decoder_input != <span class="variable language_">self</span>.pad_token).unsqueeze(<span class="number">0</span>).<span class="built_in">int</span>() &amp; causal_mask(decoder_input.size(<span class="number">0</span>)), <span class="comment"># (1, seq_len) &amp; (1, seq_len, seq_len),</span></span><br><span class="line">    <span class="comment">#解码器，我们需要一个特殊的掩码，即因果掩码。意味着每个单词只能查看前面的单词，每个单词只能查看未知的填充单词，因此我们不希望填充标记参与自注意力机制，我们只希望真实的单词参与其中，并且我们也不希望每个单词都关注其后面的单词，而只关注其前面的单词，因此我将在这里使用一种称为因果掩码的方法稍后我们会构建它。</span></span><br><span class="line">    <span class="string">&quot;label&quot;</span>: label,  <span class="comment"># (seq_len)</span></span><br><span class="line">    <span class="string">&quot;src_text&quot;</span>: src_text,</span><br><span class="line">    <span class="string">&quot;tgt_text&quot;</span>: tgt_text,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>返回由编码器输入组成的字典，包含：</p>
<ul>
<li><code>encoder_input</code> 和 <code>decoder_input</code>：前面拼好的整型张量；</li>
<li><code>encoder_mask</code>：对 <code>encoder_input</code> 中非 <code>[PAD]</code> 的位置置 1，shape 为 <code>(1,1,seq_len)</code>，用于 self-attention。</li>
<li><code>decoder_mask</code>：先对非 <code>[PAD]</code> 位置置 1，得到 <code>(1,seq_len)</code>，再与 <code>causal_mask</code>（下述函数生成的因果遮挡矩阵）按位 AND，得到 <code>(1,seq_len,seq_len)</code>，用于 Transformer 解码器的自回归限制。</li>
<li><code>label</code>：训练用的目标序列；</li>
<li><code>src_text</code>、<code>tgt_text</code>：原始文本，方便后续打印或调试。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">causal_mask</span>(<span class="params">size</span>):</span><br><span class="line">    mask = torch.triu(torch.ones((<span class="number">1</span>, size, size)), diagonal=<span class="number">1</span>).<span class="built_in">type</span>(torch.<span class="built_in">int</span>)</span><br><span class="line">    <span class="comment">#这个方法将返回对角线上方的每个值，其他所有值都将变为零，所以我们想要对角线的一种类型，我们希望它是整数，我们要做的是返回掩码等于零，所以这将返回对角线上方的所有值，对角线下方的所有值都将变为零，我们实际上想要相反的结果，即零应该会在这个表达式中变为真，所有非0的值都会变为假</span></span><br><span class="line">    <span class="keyword">return</span> mask == <span class="number">0</span></span><br></pre></td></tr></table></figure>

<p><strong>causal_mask</strong>：生成一个上三角全 1、主对角线以上（<code>diagonal=1</code>）为 1 的张量，然后取反得到下三角及对角线位置为 <code>True</code>，上三角为 <code>False</code>，用于在解码时屏蔽未来位置。</p>
<p>这段代码实现了一个用于序列到序列（sequence‑to‑sequence）机器翻译任务的数据集类 <code>BilingualDataset</code>。它将原始的双语文本对：</p>
<ol>
<li><strong>分词</strong> → 得到整数 ID 列表</li>
<li><strong>添加特殊标记</strong> <code>[SOS]</code>, <code>[EOS]</code> 和 <code>[PAD]</code> → 统一成固定长度</li>
<li><strong>构造注意力掩码</strong> → Encoder 掩掉 PAD，Decoder 同时掩掉 PAD 和未来 token</li>
<li><strong>返回模型所需的输入格式</strong>（包括 <code>encoder_input</code>、<code>decoder_input</code>、注意力掩码、以及训练标签）</li>
</ol>
<p>从而能够直接喂给基于Transformer的翻译模型进行训练或推理。</p>
<h2 id="Config"><a href="#Config" class="headerlink" title="Config"></a>Config</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_config</span>():</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;batch_size&quot;</span>: <span class="number">8</span>,</span><br><span class="line">        <span class="string">&quot;num_epochs&quot;</span>: <span class="number">20</span>,</span><br><span class="line">        <span class="string">&quot;lr&quot;</span>: <span class="number">10</span>**-<span class="number">4</span>,</span><br><span class="line">        <span class="string">&quot;seq_len&quot;</span>: <span class="number">350</span>,</span><br><span class="line">        <span class="string">&quot;d_model&quot;</span>: <span class="number">512</span>,</span><br><span class="line">        <span class="string">&quot;datasource&quot;</span>: <span class="string">&#x27;opus_books&#x27;</span>,</span><br><span class="line">        <span class="string">&quot;lang_src&quot;</span>: <span class="string">&quot;en&quot;</span>,</span><br><span class="line">        <span class="string">&quot;lang_tgt&quot;</span>: <span class="string">&quot;it&quot;</span>,</span><br><span class="line">        <span class="string">&quot;model_folder&quot;</span>: <span class="string">&quot;weights&quot;</span>,</span><br><span class="line">        <span class="string">&quot;model_basename&quot;</span>: <span class="string">&quot;tmodel_&quot;</span>,</span><br><span class="line">        <span class="string">&quot;preload&quot;</span>: <span class="string">&quot;latest&quot;</span>,</span><br><span class="line">        <span class="string">&quot;tokenizer_file&quot;</span>: <span class="string">&quot;tokenizer_&#123;0&#125;.json&quot;</span>,</span><br><span class="line">        <span class="string">&quot;experiment_name&quot;</span>: <span class="string">&quot;runs/tmodel&quot;</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p><code>get_config</code>：返回一个字典对象，包含模型训练所需的各种超参数和文件配置。</p>
<p><code>batch_size</code>：每批次处理8条样本</p>
<p><code>num_epochs</code>：训练轮数为20</p>
<p><code>lr</code>：学习率设置为0.0001</p>
<p><code>seq_len</code>：每个输入序列最大长度为350</p>
<p><code>d_model</code>：Transformer模型的隐藏维度为512</p>
<p><code>datasource</code>：数据源名，便于标识不同数据集（此处是 <code>opus_books</code>）</p>
<p><code>lang_src</code> 和 <code>lang_tgt</code>：源语言和目标语言（如从英语翻译到意大利语）</p>
<p><code>model_folder</code>：保存模型权重的文件夹（如 <code>weights</code>）</p>
<p><code>model_basename</code>：模型文件的前缀名（如 <code>tmodel_5.pt</code>）</p>
<p><code>preload</code>：加载哪个权重（”latest” 代表自动找最新的）</p>
<p><code>tokenizer_file</code>：分词器的文件名模板</p>
<p><code>experiment_name</code>：实验记录的路径（如TensorBoard的日志）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_weights_file_path</span>(<span class="params">config, epoch: <span class="built_in">str</span></span>):</span><br><span class="line">    model_folder = <span class="string">f&quot;<span class="subst">&#123;config[<span class="string">&#x27;datasource&#x27;</span>]&#125;</span>_<span class="subst">&#123;config[<span class="string">&#x27;model_folder&#x27;</span>]&#125;</span>&quot;</span>  <span class="comment"># 拼接成目录名</span></span><br><span class="line">    model_filename = <span class="string">f&quot;<span class="subst">&#123;config[<span class="string">&#x27;model_basename&#x27;</span>]&#125;</span><span class="subst">&#123;epoch&#125;</span>.pt&quot;</span>  <span class="comment"># 拼接模型文件名</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">str</span>(Path(<span class="string">&#x27;.&#x27;</span>) / model_folder / model_filename)  <span class="comment"># 返回完整路径字符串</span></span><br></pre></td></tr></table></figure>

<p><code>get_weights_file_path</code>：根据配置和给定的<code>epoch</code>数，生成当前epoch模型文件的完整路径。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">latest_weights_file_path</span>(<span class="params">config</span>):</span><br><span class="line">    model_folder = <span class="string">f&quot;<span class="subst">&#123;config[<span class="string">&#x27;datasource&#x27;</span>]&#125;</span>_<span class="subst">&#123;config[<span class="string">&#x27;model_folder&#x27;</span>]&#125;</span>&quot;</span>  <span class="comment">#权重文件所在的目录</span></span><br><span class="line">    model_filename = <span class="string">f&quot;<span class="subst">&#123;config[<span class="string">&#x27;model_basename&#x27;</span>]&#125;</span>*&quot;</span>  <span class="comment">#匹配所有模型文件</span></span><br><span class="line">    weights_files = <span class="built_in">list</span>(Path(model_folder).glob(model_filename))  <span class="comment"># 用glob匹配目录中所有模型文件</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(weights_files) == <span class="number">0</span>:  <span class="comment">#如果没有模型文件</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    weights_files.sort()  <span class="comment">#按名称排序</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">str</span>(weights_files[-<span class="number">1</span>])  <span class="comment">#返回最新的那个（排序最后一个）</span></span><br></pre></td></tr></table></figure>

<p><code>latest_weights_file_path</code>：查找给定目录下最新（最后一个按名字排序）的模型权重文件的完整路径。如果没有任何权重文件，则返回 <code>None</code>。</p>
<p>这段代码提供了训练和管理模型的一套<strong>配置信息管理工具</strong>。主要实现了以下功能：</p>
<p>用 <code>get_config()</code> 函数集中定义训练参数（如batch size、学习率、语言设置、模型文件名格式等）。</p>
<p>提供 <code>get_weights_file_path()</code> 和 <code>latest_weights_file_path()</code> 两个函数来<strong>动态生成模型权重文件的保存路径或加载路径</strong>，支持按照epoch命名和获取最新模型。</p>
<p>这个模块的设计非常适合用于训练循环中管理模型的保存和加载行为，是构建机器学习训练框架的重要一部分。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://Harrisonls2004.github.io">Harrisonls2004</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://harrisonls2004.github.io/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/">https://harrisonls2004.github.io/2025/03/28/Transformer%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://Harrisonls2004.github.io" target="_blank">lhldudu's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Trasformer/">Trasformer</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/%E6%89%A9%E5%B1%95%E5%AD%A6%E4%B9%A0/">扩展学习</a></div><div class="post-share"><div class="social-share" data-image="/img/Transformer.jpg" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/05/25/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9FOpenEuler%E5%AE%9E%E9%AA%8C/" title="操作系统OpenEuler实验"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/os.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">操作系统OpenEuler实验</div></div><div class="info-2"><div class="info-item-1">操作系统OpenEuler实验实验内容与环境实验内容完成openEuler操作系统的安装，完成内核更新，在此基础上，增加完成其他功能。本次实验已增添完成：内核模块编程、内存管理、内核时间管理。 实验环境虚拟机Vmware和openEuler操作系统。 实验过程openEuler操作系统安装1.下载OpenEular镜像   2.安装到虚拟机 （1）新建虚拟机               （2）进入虚拟机，开始安装  ①等待安装检查   ②进行语言设置：选择中文  ③进行软件选择：这里我们选择最小安装，附加选项为标准和开发工具   ④设置ROOT密码：12Lhl0408  ⑤创建用户：设置...</div></div></div></a></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="waline-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Harrisonls2004</div><div class="author-info-description">尽人事 听天命</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">30</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Harrisonls2004/"><i class="fab fa-github"></i><span>关注我</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/Harrisonls2004" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:lhldudu@bupt.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://qm.qq.com/q/1030824397" target="_blank" title="QQ"><i class="fab fa-qq" style="color: #12b7f5;"></i></a><a class="social-icon" href="/img/wx.jpg" target="_blank" title="微信"><i class="fab fa-weixin" style="color: #09b83e;"></i></a><a class="social-icon" href="https://www.douyin.com/user/MS4wLjABAAAAn_2JnST2JJZLzTBjHHu28he2MZut6di0JkJcxc0-_uA?from_tab_name=main" target="_blank" title="抖音"><i class="fab fa-tiktok" style="color: #000000;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客！分享技术与生活感悟。有问题可以私信联系我，我会定期更新哒~</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text">Transformer简介</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81Transformer%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.</span> <span class="toc-text">为什么需要Transformer模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A0%B8%E5%BF%83%E7%89%B9%E7%82%B9"><span class="toc-number">1.2.</span> <span class="toc-text">Transformer模型的核心特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A0%B8%E5%BF%83%E7%89%B9%E7%82%B9-1"><span class="toc-number">1.3.</span> <span class="toc-text">Transformer模型的核心特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E9%83%A8%E6%9C%BA%E5%88%B6%E6%A6%82%E8%BF%B0"><span class="toc-number">1.4.</span> <span class="toc-text">内部机制概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Encoder%EF%BC%88%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%89"><span class="toc-number">1.5.</span> <span class="toc-text">Encoder（编码器）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Decoder%EF%BC%88%E8%A7%A3%E7%A0%81%E5%99%A8%EF%BC%89"><span class="toc-number">1.6.</span> <span class="toc-text">Decoder（解码器）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.</span> <span class="toc-text">Transformer模型实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E5%B5%8C%E5%85%A5%EF%BC%88Input-Embeddings%EF%BC%89"><span class="toc-number">2.1.</span> <span class="toc-text">输入嵌入（Input Embeddings）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%EF%BC%88Positional-Encoding%EF%BC%89"><span class="toc-number">2.2.</span> <span class="toc-text">位置编码（Positional Encoding）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%88Layer-Normalization%EF%BC%89"><span class="toc-number">2.3.</span> <span class="toc-text">层归一化（Layer Normalization）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88Feed-Forward-Network%EF%BC%89"><span class="toc-number">2.4.</span> <span class="toc-text">前馈神经网络（Feed Forward Network）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%88Multi-Head-Attention%EF%BC%89"><span class="toc-number">2.5.</span> <span class="toc-text">多头注意力机制（Multi-Head Attention）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%EF%BC%88Residual-Connection%EF%BC%89"><span class="toc-number">2.6.</span> <span class="toc-text">残差连接（Residual Connection）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8%E5%9D%97%EF%BC%88Encoder-Block%EF%BC%89"><span class="toc-number">2.7.</span> <span class="toc-text">编码器块（Encoder Block）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%88Encoder%EF%BC%89"><span class="toc-number">2.8.</span> <span class="toc-text">完整编码器（Encoder）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8%E5%9D%97%EF%BC%88Decoder-Block%EF%BC%89"><span class="toc-number">2.9.</span> <span class="toc-text">解码器块（Decoder Block）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E8%A7%A3%E7%A0%81%E5%99%A8%EF%BC%88Decoder%EF%BC%89"><span class="toc-number">2.10.</span> <span class="toc-text">完整解码器（Decoder）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8A%95%E5%BD%B1%E5%B1%82%EF%BC%88Projection-Layer%EF%BC%89"><span class="toc-number">2.11.</span> <span class="toc-text">投影层（Projection Layer）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4Transformer%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.12.</span> <span class="toc-text">完整Transformer模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA%E5%87%BD%E6%95%B0"><span class="toc-number">2.13.</span> <span class="toc-text">模型构建函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer%E8%AE%AD%E7%BB%83%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text">Transformer训练实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E5%85%A5%E4%BE%9D%E8%B5%96%E5%BA%93"><span class="toc-number">3.1.</span> <span class="toc-text">导入依赖库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%AA%E5%BF%83%E8%A7%A3%E7%A0%81%E5%87%BD%E6%95%B0"><span class="toc-number">3.2.</span> <span class="toc-text">贪心解码函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E5%87%BD%E6%95%B0"><span class="toc-number">3.3.</span> <span class="toc-text">验证函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%87%BD%E6%95%B0"><span class="toc-number">3.4.</span> <span class="toc-text">数据处理函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E4%B8%BB%E5%87%BD%E6%95%B0"><span class="toc-number">3.5.</span> <span class="toc-text">训练主函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">4.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dataset"><span class="toc-number">5.</span> <span class="toc-text">Dataset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Config"><span class="toc-number">6.</span> <span class="toc-text">Config</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2026/01/27/EvoCorps/" title="EvoCorps"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/logo1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="EvoCorps"/></a><div class="content"><a class="title" href="/2026/01/27/EvoCorps/" title="EvoCorps">EvoCorps</a><time datetime="2026-01-27T05:00:00.000Z" title="发表于 2026-01-27 13:00:00">2026-01-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/01/25/%E5%8C%97%E9%82%AE%E7%BD%91%E5%AE%89%E5%A4%A7%E7%B1%BB%E8%AF%BE%E7%A8%8B%E4%BB%8B%E7%BB%8D/" title="北邮网安课程介绍（23级）"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/buptnew.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="北邮网安课程介绍（23级）"/></a><div class="content"><a class="title" href="/2026/01/25/%E5%8C%97%E9%82%AE%E7%BD%91%E5%AE%89%E5%A4%A7%E7%B1%BB%E8%AF%BE%E7%A8%8B%E4%BB%8B%E7%BB%8D/" title="北邮网安课程介绍（23级）">北邮网安课程介绍（23级）</a><time datetime="2026-01-25T11:48:17.000Z" title="发表于 2026-01-25 19:48:17">2026-01-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/01/06/%E8%BD%AF%E4%BB%B6%E5%AE%89%E5%85%A8%E6%AF%94%E8%BE%83%E9%87%8D%E8%A6%81%E7%9A%84%E5%9C%B0%E6%96%B9/" title="软件安全期末复习"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/softsec.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="软件安全期末复习"/></a><div class="content"><a class="title" href="/2026/01/06/%E8%BD%AF%E4%BB%B6%E5%AE%89%E5%85%A8%E6%AF%94%E8%BE%83%E9%87%8D%E8%A6%81%E7%9A%84%E5%9C%B0%E6%96%B9/" title="软件安全期末复习">软件安全期末复习</a><time datetime="2026-01-06T05:00:00.000Z" title="发表于 2026-01-06 13:00:00">2026-01-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/31/2025%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93%E4%B8%8B/" title="2025年度下半年总结"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/2025.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025年度下半年总结"/></a><div class="content"><a class="title" href="/2025/12/31/2025%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93%E4%B8%8B/" title="2025年度下半年总结">2025年度下半年总结</a><time datetime="2025-12-31T07:00:00.000Z" title="发表于 2025-12-31 15:00:00">2025-12-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/31/2025%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93%E4%B8%8A/" title="2025年度上半年总结"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/2025.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2025年度上半年总结"/></a><div class="content"><a class="title" href="/2025/12/31/2025%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93%E4%B8%8A/" title="2025年度上半年总结">2025年度上半年总结</a><time datetime="2025-12-31T05:00:00.000Z" title="发表于 2025-12-31 13:00:00">2025-12-31</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By Harrisonls2004</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4-b1</a></span></div><div class="footer_custom_text">尽人事 听天命</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4-b1"></script><script src="/js/main.js?v=5.5.4-b1"></script><script src="/js/tw_cn.js?v=5.5.4-b1"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.7/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.js"></script><div class="js-pjax"><script>(() => {
  let initFn = window.walineFn || null
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = {"lang":"zh-CN","locale":{"placeholder":"开始你的表演，请文明发言哦！"},"emoji":["https://unpkg.com/@waline/emojis@1.2.0/weibo","https://unpkg.com/@waline/emojis@1.2.0/alus","https://unpkg.com/@waline/emojis@1.2.0/bilibili"],"meta":["nick","mail","link"],"requiredMeta":[],"wordLimit":500,"pageSize":10,"avatar":"monsterid","avatarCDN":"https://sdn.geekzu.org/avatar/","avatarForce":false,"highlight":true,"mathTagSupport":true,"commentSorting":"latest"}

  const destroyWaline = ele => ele.destroy()

  const initWaline = (Fn, el = document, path = window.location.pathname) => {
    const waline = Fn({
      el: el.querySelector('#waline-wrap'),
      serverURL: 'https://waline-chi-beryl.vercel.app',
      pageview: true,
      dark: 'html[data-theme="dark"]',
      comment: true,
      ...option,
      path: isShuoshuo ? path : (option && option.path) || path
    })

    if (isShuoshuo) {
      window.shuoshuoComment.destroyWaline = () => {
        destroyWaline(waline)
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const loadWaline = (el, path) => {
    if (initFn) initWaline(initFn, el, path)
    else {
      btf.getCSS('https://cdn.jsdelivr.net/npm/@waline/client@3.8.0/dist/waline.min.css')
        .then(() => import('https://cdn.jsdelivr.net/npm/@waline/client@3.8.0/dist/waline.min.js'))
        .then(({ init }) => {
          initFn = init || Waline.init
          initWaline(initFn, el, path)
          window.walineFn = initFn
        })
    }
  }

  if (isShuoshuo) {
    'Waline' === 'Waline'
      ? window.shuoshuoComment = { loadComment: loadWaline } 
      : window.loadOtherComment = loadWaline
    return
  }

  if ('Waline' === 'Waline' || !false) {
    if (false) btf.loadComment(document.getElementById('waline-wrap'),loadWaline)
    else setTimeout(loadWaline, 0)
  } else {
    window.loadOtherComment = loadWaline
  }
})()</script></div><script src="/js/click-heart.js"></script><script src="/js/sakura.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/metingjs/dist/Meting.min.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章..." type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.4-b1"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"right","width":150,"height":300,"hOffset":0,"vOffset":-20},"mobile":{"show":true,"scale":0.5},"react":{"opacity":0.7},"log":false});</script></body></html>